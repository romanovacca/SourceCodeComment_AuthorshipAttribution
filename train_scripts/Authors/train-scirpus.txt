import pandas as pd 
import numpy as np 
import xgboost as xgb
from scipy.optimize import fmin_powell
from ml_metrics import quadratic_weighted_kappa


def kapparegobj(preds, dtrain):
    labels = dtrain.get_label()
    x = (preds - labels)
    grad = 2*x*np.exp(-(x**2))*(np.exp(x**2)+x**2+1)
    hess = 2*np.exp(-(x**2))*(np.exp(x**2)-2*(x**4)+5*(x**2)-1)
    return grad, hess


def kappaerror(preds, dtrain):
    labels = dtrain.get_label()
    x = (labels-preds)
    error = (x**2)*(1-np.exp(-(x**2)))
    return 'error', np.mean(error)
    

def eval_wrapper(yhat, y):  
    y = np.array(y)
    y = y.astype(int)
    yhat = np.array(yhat)
    yhat = np.clip(np.round(yhat), np.min(y), np.max(y)).astype(int)   
    return quadratic_weighted_kappa(yhat, y)
    
def get_params():
    
    params = {}
    params["objective"] = "reg:linear"     
    params["eta"] = 0.05
    params["min_child_weight"] = 240
    params["subsample"] = 0.9
    params["colsample_bytree"] = 0.67
    params["silent"] = 1
    params["max_depth"] = 6
    plst = list(params.items())

    return plst
    
def apply_offset(data, bin_offset, sv, scorer=eval_wrapper):
    # data has the format of pred=0, offset_pred=1, labels=2 in the first dim
    data[1, data[0].astype(int)==sv] = data[0, data[0].astype(int)==sv] + bin_offset
    score = scorer(data[1], data[2])
    return score

# global variables
columns_to_drop = ['Id', 'Response', 'Medical_History_10','Medical_History_24']
xgb_num_rounds = 800
num_classes = 8
eta_list = [0.05] * 200 
eta_list = eta_list + [0.02] * 500
eat_list = eta_list + [0.01]*100

print("Load the data using pandas")
train = pd.read_csv("../input/train.csv")
test = pd.read_csv("../input/test.csv")

# combine train and test
all_data = train.append(test)

# create any new variables    
all_data['Product_Info_2_char'] = all_data.Product_Info_2.str[0]
all_data['Product_Info_2_num'] = all_data.Product_Info_2.str[1]

# factorize categorical variables
all_data['Product_Info_2'] = pd.factorize(all_data['Product_Info_2'])[0]
all_data['Product_Info_2_char'] = pd.factorize(all_data['Product_Info_2_char'])[0]
all_data['Product_Info_2_num'] = pd.factorize(all_data['Product_Info_2_num'])[0]

all_data['BMI_Age'] = all_data['BMI'] * all_data['Ins_Age']

med_keyword_columns = all_data.columns[all_data.columns.str.startswith('Medical_Keyword_')]
all_data['Med_Keywords_Count'] = all_data[med_keyword_columns].sum(axis=1)

print('Eliminate missing values')    
# Use -1 for any others
all_data.fillna(-1, inplace=True)

# fix the dtype on the label column
all_data['Response'] = all_data['Response'].astype(int)

# split train and test
train = all_data[all_data['Response']>0].copy()
test = all_data[all_data['Response']<1].copy()

# convert data to xgb data structure
xgtrain = xgb.DMatrix(train.drop(columns_to_drop, axis=1), train['Response'].values)
xgtest = xgb.DMatrix(test.drop(columns_to_drop, axis=1), label=test['Response'].values)    

# get the parameters for xgboost
plst = get_params()
print(plst)      

# train model
model = xgb.train(plst, xgtrain, xgb_num_rounds, obj=kapparegobj, feval=kappaerror, learning_rates=eta_list) 

# get preds
train_preds = model.predict(xgtrain, ntree_limit=model.best_iteration)
print('Train score is:', eval_wrapper(train_preds, train['Response'])) 
test_preds = model.predict(xgtest, ntree_limit=model.best_iteration)
train_preds = np.clip(train_preds, -0.99, 8.99)
test_preds = np.clip(test_preds, -0.99, 8.99)

# train offsets 
offsets = np.array([0.1, -1, -2, -1, -0.8, 0.02, 0.8, 1])
data = np.vstack((train_preds, train_preds, train['Response'].values))
for j in range(num_classes):
    data[1, data[0].astype(int)==j] = data[0, data[0].astype(int)==j] + offsets[j] 
for j in range(num_classes):
    train_offset = lambda x: -apply_offset(data, x, j)
    offsets[j] = fmin_powell(train_offset, offsets[j])  

# apply offsets to test
data = np.vstack((test_preds, test_preds, test['Response'].values))
for j in range(num_classes):
    data[1, data[0].astype(int)==j] = data[0, data[0].astype(int)==j] + offsets[j] 

final_test_preds = np.round(np.clip(data[1], 1, 8)).astype(int)

preds_out = pd.DataFrame({"Id": test['Id'].values, "Response": final_test_preds})
preds_out = preds_out.set_index('Id')
preds_out.to_csv('xgb_offset_submission_2.csv')

df = pd.read_csv("../input/data.csv")
df.drop(['game_event_id', 'game_id', 'lat', 'lon', 'team_id', 'team_name'],
        axis=1,
        inplace=True)
df.sort_values('game_date',  inplace=True)
mask = df['shot_made_flag'].isnull()

# Clean data
actiontypes = dict(df.action_type.value_counts())
df['type'] = \
    df.apply(lambda row: (row['action_type']
                          if actiontypes[row['action_type']] > 20
                          else row['combined_shot_type']), axis=1)
df.drop(['action_type', 'combined_shot_type'], axis=1, inplace=True)

df['away'] = df.matchup.str.contains('@')
df.drop('matchup', axis=1, inplace=True)

df['distance'] = df.apply(lambda row: row['shot_distance']
                          if row['shot_distance'] < 45 else 45, axis=1)

df['time_remaining'] = \
    df.apply(lambda x: x['minutes_remaining'] * 60 + x['seconds_remaining'],
             axis=1)
df['last_moments'] = \
    df.apply(lambda row: 1 if row['time_remaining'] < 3 else 0, axis=1)

data = pd.get_dummies(df['type'], prefix="action_type")

features = ["away", "period", "playoffs", "shot_type", "shot_zone_area",
            "shot_zone_basic", "season", "shot_zone_range", "opponent",
            "distance", "minutes_remaining", "last_moments"]
for f in features:
    data = pd.concat([data, pd.get_dummies(df[f], prefix=f), ], axis=1)
ss = StandardScaler()
train = data[~mask].copy()
features = train.columns
train[features] = np.round(ss.fit_transform(train[features]), 6)
train['shot_made_flag'] = df.shot_made_flag[~mask]
test = data[mask].copy()
test.insert(0, 'shot_id', df[mask].shot_id)
test[features] = np.round(ss.transform(test[features]), 6)
trainpredictions1 = GPIndividual1(train)
trainpredictions2 = GPIndividual2(train)
trainpredictions3 = GPIndividual3(train)
trainpredictions4 = GPIndividual4(train)
trainpredictions5 = GPIndividual5(train)
testpredictions1 = GPIndividual1(test)
testpredictions2 = GPIndividual2(test)
testpredictions3 = GPIndividual3(test)
testpredictions4 = GPIndividual4(test)
testpredictions5 = GPIndividual5(test)
predictions = (trainpredictions1 +
               trainpredictions2 +
               trainpredictions3 +
               trainpredictions4 +
               trainpredictions5)/5

print(log_loss(train.shot_made_flag.values, predictions.values))

predictions = (testpredictions1 +
               testpredictions2 +
               testpredictions3 +
               testpredictions4 +
               testpredictions5)/5

submission = pd.DataFrame({"shot_id": test.shot_id,
                           "shot_made_flag": predictions})
submission.sort_values('shot_id',  inplace=True)
submission.to_csv("arisubmission.csv", index=False)

predictions = np.power(trainpredictions1 *
                       trainpredictions2 *
                       trainpredictions3 *
                       trainpredictions4 *
                       trainpredictions5, 1./5)

print(log_loss(train.shot_made_flag.values, predictions.values))

predictions = np.power(testpredictions1 *
                       testpredictions2 *
                       testpredictions3 *
                       testpredictions4 *
                       testpredictions5, 1./5)

submission = pd.DataFrame({"shot_id": test.shot_id,
                           "shot_made_flag": predictions})
submission.sort_values('shot_id',  inplace=True)
submission.to_csv("geosubmission.csv", index=False)

from heapq import nlargest
from operator import itemgetter
from collections import defaultdict


def run_solution():
    
    print('Preparing arrays...')
    f = open("../input/train.csv", "r")
    f.readline()
    best_hotels_od_ulc = defaultdict(lambda: defaultdict(int))
    best_hotels_search_dest = defaultdict(lambda: defaultdict(int))
    best_hotels_search_dest1 = defaultdict(lambda: defaultdict(int))
    best_hotel_country = defaultdict(lambda: defaultdict(int))
    popular_hotel_cluster = defaultdict(int)
    total = 0

    # Calc counts
    while 1:
        line = f.readline().strip()
        total += 1

        if total % 10000000 == 0:
            print('Read {} lines...'.format(total))

        if line == '':
            break

        arr = line.split(",")
        book_year = int(arr[0][:4])
        user_location_city = arr[5]
        orig_destination_distance = arr[6]
        srch_destination_id = arr[16]
        is_booking = int(arr[18])
        hotel_country = arr[21]
        hotel_market = arr[22]
        hotel_cluster = arr[23]

        append_1 = 3 + 12*is_booking
        append_2 = 3 + 5*is_booking

        if user_location_city != '' and orig_destination_distance != '':
            hsh = (hash('user_location_city_'+str(user_location_city) +
                        '_orig_destination_distance_' +
                        str(orig_destination_distance)))
            best_hotels_od_ulc[hsh][hotel_cluster] += 1

        if srch_destination_id != '' and hotel_country != '' \
                and hotel_market != '' and book_year == 2014:
            hsh = (hash('srch_destination_id_' +
                        str(srch_destination_id) +
                        "_hotel_country_" + str(hotel_country) +
                        "_hotel_market_"+str(hotel_market)))
            best_hotels_search_dest[hsh][hotel_cluster] += append_1
        if srch_destination_id != '':
            hsh = hash('srch_destination_id_'+str(srch_destination_id))
            best_hotels_search_dest1[hsh][hotel_cluster] += append_1
        if hotel_country != '':
            hsh = hash('hotel_country_'+str(hotel_country))
            best_hotel_country[hsh][hotel_cluster] += append_2
        popular_hotel_cluster[hotel_cluster] += 1
    f.close()

    print('Generate submission...')
    path = 'lowsubmission.csv'
    out = open(path, "w")
    f = open("../input/test.csv", "r")
    f.readline()
    total = 0
    out.write("id,hotel_cluster\n")
    topclusters = nlargest(5, sorted(popular_hotel_cluster.items()),
                           key=itemgetter(1))

    while 1:
        line = f.readline().strip()
        total += 1

        if total % 1000000 == 0:
            print('Write {} lines...'.format(total))

        if line == '':
            break

        arr = line.split(",")
        id = arr[0]
        user_location_city = arr[6]
        orig_destination_distance = arr[7]
        srch_destination_id = arr[17]
        hotel_country = arr[20]
        hotel_market = arr[21]

        out.write(str(id) + ',')
        filled = []

        hsh = (hash('user_location_city_'+str(user_location_city) +
                    '_orig_destination_distance_' +
                    str(orig_destination_distance)))
        if hsh in best_hotels_od_ulc:
            d = best_hotels_od_ulc[hsh]
            topitems = nlargest(5, sorted(d.items()), key=itemgetter(1))
            for i in range(len(topitems)):
                if topitems[i][0] in filled:
                    continue
                if len(filled) == 5:
                    break
                out.write(' ' + topitems[i][0])
                filled.append(topitems[i][0])

        hsh1 = (hash('srch_destination_id_' +
                     str(srch_destination_id) +
                     "_hotel_country_" + str(hotel_country) +
                     "_hotel_market_"+str(hotel_market)))
        hsh2 = hash('srch_destination_id_'+str(srch_destination_id))
        if (len(filled) < 5) and (hsh1 in best_hotels_search_dest):
            d = best_hotels_search_dest[hsh1]
            topitems = nlargest(5, d.items(), key=itemgetter(1))
            for i in range(len(topitems)):
                if topitems[i][0] in filled:
                    continue
                if len(filled) == 5:
                    break
                out.write(' ' + topitems[i][0])
                filled.append(topitems[i][0])
        elif (len(filled) < 5) and (hsh2 in best_hotels_search_dest1):
            d = best_hotels_search_dest1[hsh2]
            topitems = nlargest(5, d.items(), key=itemgetter(1))
            for i in range(len(topitems)):
                if topitems[i][0] in filled:
                    continue
                if len(filled) == 5:
                    break
                out.write(' ' + topitems[i][0])
                filled.append(topitems[i][0])

        hsh = hash('hotel_country_'+str(hotel_country))
        if (len(filled) < 5) and (hsh in best_hotel_country):
            d = best_hotel_country[hsh]
            topitems = nlargest(5, d.items(), key=itemgetter(1))
            for i in range(len(topitems)):
                if topitems[i][0] in filled:
                    continue
                if len(filled) == 5:
                    break
                out.write(' ' + topitems[i][0])
                filled.append(topitems[i][0])

        if(len(filled) < 5):
            for i in range(len(topclusters)):
                if topclusters[i][0] in filled:
                    continue
                if len(filled) == 5:
                    break
                out.write(' ' + topclusters[i][0])
                filled.append(topclusters[i][0])

        out.write("\n")
    out.close()
    print('Completed!')


if __name__ == "__main__":
    run_solution()